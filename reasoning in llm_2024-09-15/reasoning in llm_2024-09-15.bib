@misc{Windows Agent Arena_ Evaluating Multi-Modal OS Agents at Scale_2024,
  title = {{ Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale }},
  author = {{ Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Justin Wagle, Kazuhito Koishida, Arthur Bucker, Lawrence Jang, Zack Hui }},
  year = {{ 2024 }},
  eprint = {{ 2409.08264v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning. However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks. To address these challenges, we introduce the Windows Agent Arena: a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage. Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we also introduce a new multi-modal agent, Navi. Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human. Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis of Navi's performance, and provide insights into the opportunities for future research in agent development and data generation using Windows Agent Arena. Webpage: https://microsoft.github.io/WindowsAgentArena Code: https://github.com/microsoft/WindowsAgentArena }},
  url = {{ https://arxiv.org/abs/2409.08264v1 }}
}

@misc{OmniQuery_ Contextually Augmenting Captured Multimodal Memory to Enable_  Personal Question Answering_2024,
  title = {{ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable
  Personal Question Answering }},
  author = {{ Jiahao Nick Li, Zhuohao Jerry Zhang, Jiaju Ma }},
  year = {{ 2024 }},
  eprint = {{ 2409.08250v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ People often capture memories through photos, screenshots, and videos. While existing AI-based tools enable querying this data using natural language, they mostly only support retrieving individual pieces of information like certain objects in photos and struggle with answering more complex queries that involve interpreting interconnected memories like event sequences. We conducted a one-month diary study to collect realistic user queries and generated a taxonomy of necessary contextual information for integrating with captured memories. We then introduce OmniQuery, a novel system that is able to answer complex personal memory-related questions that require extracting and inferring contextual information. OmniQuery augments single captured memories through integrating scattered contextual information from multiple interconnected memories, retrieves relevant memories, and uses a large language model (LLM) to comprehensive answers. In human evaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG system, winning or tying in 74.5% of the time. }},
  url = {{ https://arxiv.org/abs/2409.08250v1 }}
}

@misc{Source2Synth_ Synthetic Data Generation and Curation Grounded in Real_  Data Sources_2024,
  title = {{ Source2Synth: Synthetic Data Generation and Curation Grounded in Real
  Data Sources }},
  author = {{ Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli }},
  year = {{ 2024 }},
  eprint = {{ 2409.08239v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines. }},
  url = {{ https://arxiv.org/abs/2409.08239v1 }}
}

@misc{LLM Honeypot_ Leveraging Large Language Models as Advanced Interactive_  Honeypot Systems_2024,
  title = {{ LLM Honeypot: Leveraging Large Language Models as Advanced Interactive
  Honeypot Systems }},
  author = {{ Hakan T. Otal, M. Abdullah Canbaz }},
  year = {{ 2024 }},
  eprint = {{ 2409.08234v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ The rapid evolution of cyber threats necessitates innovative solutions for detecting and analyzing malicious activity. Honeypots, which are decoy systems designed to lure and interact with attackers, have emerged as a critical component in cybersecurity. In this paper, we present a novel approach to creating realistic and interactive honeypot systems using Large Language Models (LLMs). By fine-tuning a pre-trained open-source language model on a diverse dataset of attacker-generated commands and responses, we developed a honeypot capable of sophisticated engagement with attackers. Our methodology involved several key steps: data collection and processing, prompt engineering, model selection, and supervised fine-tuning to optimize the model's performance. Evaluation through similarity metrics and live deployment demonstrated that our approach effectively generates accurate and informative responses. The results highlight the potential of LLMs to revolutionize honeypot technology, providing cybersecurity professionals with a powerful tool to detect and analyze malicious activity, thereby enhancing overall security infrastructure. }},
  url = {{ https://arxiv.org/abs/2409.08234v1 }}
}

@misc{Fine-tuning Large Language Models for Entity Matching_2024,
  title = {{ Fine-tuning Large Language Models for Entity Matching }},
  author = {{ Aaron Steiner, Ralph Peeters, Christian Bizer }},
  year = {{ 2024 }},
  eprint = {{ 2409.08185v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and their ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) The representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the model's ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o Mini. }},
  url = {{ https://arxiv.org/abs/2409.08185v1 }}
}

@misc{Faster Speech-LLaMA Inference with Multi-token Prediction_2024,
  title = {{ Faster Speech-LLaMA Inference with Multi-token Prediction }},
  author = {{ Desh Raj, Gil Keren, Junteng Jia, Jay Mahadeokar, Ozlem Kalinli }},
  year = {{ 2024 }},
  eprint = {{ 2409.08148v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Large language models (LLMs) have become proficient at solving a wide variety of tasks, including those involving multi-modal inputs. In particular, instantiating an LLM (such as LLaMA) with a speech encoder and training it on paired data imparts speech recognition (ASR) abilities to the decoder-only model, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of auto-regressive inference and the relatively large decoder, Speech-LLaMA models require relatively high inference time. In this work, we propose to speed up Speech-LLaMA inference by predicting multiple tokens in the same decoding step. We explore several model architectures that enable this, and investigate their performance using threshold-based and verification-based inference strategies. We also propose a prefix-based beam search decoding method that allows efficient minimum word error rate (MWER) training for such models. We evaluate our models on a variety of public benchmarks, where they reduce the number of decoder calls by ~3.2x while maintaining or improving WER performance. }},
  url = {{ https://arxiv.org/abs/2409.08148v1 }}
}

@misc{LLM-POTUS Score_ A Framework of Analyzing Presidential Debates with_  Large Language Models_2024,
  title = {{ LLM-POTUS Score: A Framework of Analyzing Presidential Debates with
  Large Language Models }},
  author = {{ Zhengliang Liu, Yiwei Li, Oleksandra Zolotarevych, Rongwei Yang, Tianming Liu }},
  year = {{ 2024 }},
  eprint = {{ 2409.08147v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Large language models have demonstrated remarkable capabilities in natural language processing, yet their application to political discourse analysis remains underexplored. This paper introduces a novel approach to evaluating presidential debate performances using LLMs, addressing the longstanding challenge of objectively assessing debate outcomes. We propose a framework that analyzes candidates' "Policies, Persona, and Perspective" (3P) and how they resonate with the "Interests, Ideologies, and Identity" (3I) of four key audience groups: voters, businesses, donors, and politicians. Our method employs large language models to generate the LLM-POTUS Score, a quantitative measure of debate performance based on the alignment between 3P and 3I. We apply this framework to analyze transcripts from recent U.S. presidential debates, demonstrating its ability to provide nuanced, multi-dimensional assessments of candidate performances. Our results reveal insights into the effectiveness of different debating strategies and their impact on various audience segments. This study not only offers a new tool for political analysis but also explores the potential and limitations of using LLMs as impartial judges in complex social contexts. In addition, this framework provides individual citizens with an independent tool to evaluate presidential debate performances, which enhances democratic engagement and reduces reliance on potentially biased media interpretations and institutional influence, thereby strengthening the foundation of informed civic participation. }},
  url = {{ https://arxiv.org/abs/2409.08147v1 }}
}

@misc{The CLC-UKET Dataset_ Benchmarking Case Outcome Prediction for the UK_  Employment Tribunal_2024,
  title = {{ The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK
  Employment Tribunal }},
  author = {{ Huiyuan Xie, Felix Steffek, Joana Ribeiro de Faria, Christine Carter, Jonathan Rutherford }},
  year = {{ 2024 }},
  eprint = {{ 2409.08098v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ This paper explores the intersection of technological innovation and access to justice by developing a benchmark for predicting case outcomes in the UK Employment Tribunal (UKET). To address the challenge of extensive manual annotation, the study employs a large language model (LLM) for automatic annotation, resulting in the creation of the CLC-UKET dataset. The dataset consists of approximately 19,000 UKET cases and their metadata. Comprehensive legal annotations cover facts, claims, precedent references, statutory references, case outcomes, reasons and jurisdiction codes. Facilitated by the CLC-UKET data, we examine a multi-class case outcome prediction task in the UKET. Human predictions are collected to establish a performance reference for model comparison. Empirical results from baseline models indicate that finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET prediction task. The performance of zero-shot LLMs can be enhanced by integrating task-related information into few-shot examples. We hope that the CLC-UKET dataset, along with human annotations and empirical findings, can serve as a valuable benchmark for employment-related dispute resolution. }},
  url = {{ https://arxiv.org/abs/2409.08098v1 }}
}

@misc{Securing Large Language Models_ Addressing Bias, Misinformation, and_  Prompt Attacks_2024,
  title = {{ Securing Large Language Models: Addressing Bias, Misinformation, and
  Prompt Attacks }},
  author = {{ Benji Peng, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Junyu Liu, Qian Niu }},
  year = {{ 2024 }},
  eprint = {{ 2409.08087v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field. }},
  url = {{ https://arxiv.org/abs/2409.08087v1 }}
}

@misc{TravelAgent_ An AI Assistant for Personalized Travel Planning_2024,
  title = {{ TravelAgent: An AI Assistant for Personalized Travel Planning }},
  author = {{ Aili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, Jiangjie Chen }},
  year = {{ 2024 }},
  eprint = {{ 2409.08069v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ As global tourism expands and artificial intelligence technology advances, intelligent travel planning services have emerged as a significant research focus. Within dynamic real-world travel scenarios with multi-dimensional constraints, services that support users in automatically creating practical and customized travel itineraries must address three key objectives: Rationality, Comprehensiveness, and Personalization. However, existing systems with rule-based combinations or LLM-based planning methods struggle to fully satisfy these criteria. To overcome the challenges, we introduce TravelAgent, a travel planning system powered by large language models (LLMs) designed to provide reasonable, comprehensive, and personalized travel itineraries grounded in dynamic scenarios. TravelAgent comprises four modules: Tool-usage, Recommendation, Planning, and Memory Module. We evaluate TravelAgent's performance with human and simulated users, demonstrating its overall effectiveness in three criteria and confirming the accuracy of personalized recommendations. }},
  url = {{ https://arxiv.org/abs/2409.08069v1 }}
}

@misc{From Explanations to Action_ A Zero-Shot, Theory-Driven LLM Framework_  for Student Performance Feedback_2024,
  title = {{ From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework
  for Student Performance Feedback }},
  author = {{ Vinitra Swamy, Davide Romano, Bhargav Srinivasa Desikan, Oana-Maria Camburu, Tanja KÃ¤ser }},
  year = {{ 2024 }},
  eprint = {{ 2409.08027v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Recent advances in eXplainable AI (XAI) for education have highlighted a critical challenge: ensuring that explanations for state-of-the-art AI models are understandable for non-technical users such as educators and students. In response, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI pipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is designed to deliver theory-driven, actionable feedback to students in online courses. iLLuMinaTE navigates three main stages - causal connection, explanation selection, and explanation presentation - with variations drawing from eight social science theories (e.g. Abnormal Conditions, Pearl's Model of Explanation, Necessity and Robustness Selection, Contrastive Explanation). We extensively evaluate 21,915 natural language explanations of iLLuMinaTE extracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different underlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from three diverse online courses. Our evaluation involves analyses of explanation alignment to the social science theory, understandability of the explanation, and a real-world user preference study with 114 university students containing a novel actionability simulation. We find that students prefer iLLuMinaTE explanations over traditional explainers 89.52% of the time. Our work provides a robust, ready-to-use framework for effectively communicating hybrid XAI-driven insights in education, with significant generalization potential for other human-centric fields. }},
  url = {{ https://arxiv.org/abs/2409.08027v1 }}
}

@misc{An Evaluation Framework for Attributed Information Retrieval using Large_  Language Models_2024,
  title = {{ An Evaluation Framework for Attributed Information Retrieval using Large
  Language Models }},
  author = {{ Hanane Djeddal, Pierre Erbacher, Raouf Toukal, Laure Soulier, Karen Pinel-Sauvagnat, Sophia Katrenko, Lynda Tamine }},
  year = {{ 2024 }},
  eprint = {{ 2409.08014v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ With the growing success of Large Language models (LLMs) in information-seeking scenarios, search engines are now adopting generative approaches to provide answers along with in-line citations as attribution. While existing work focuses mainly on attributed question answering, in this paper, we target information-seeking scenarios which are often more challenging due to the open-ended nature of the queries and the size of the label space in terms of the diversity of candidate-attributed answers per query. We propose a reproducible framework to evaluate and benchmark attributed information seeking, using any backbone LLM, and different architectural designs: (1) Generate (2) Retrieve then Generate, and (3) Generate then Retrieve. Experiments using HAGRID, an attributed information-seeking dataset, show the impact of different scenarios on both the correctness and attributability of answers. }},
  url = {{ https://arxiv.org/abs/2409.08014v1 }}
}

@misc{WirelessAgent_ Large Language Model Agents for Intelligent Wireless_  Networks_2024,
  title = {{ WirelessAgent: Large Language Model Agents for Intelligent Wireless
  Networks }},
  author = {{ Jingwen Tong, Jiawei Shao, Qiong Wu, Wei Guo, Zijian Li, Zehong Lin, Jun Zhang }},
  year = {{ 2024 }},
  eprint = {{ 2409.07964v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Wireless networks are increasingly facing challenges due to their expanding scale and complexity. These challenges underscore the need for advanced AI-driven strategies, particularly in the upcoming 6G networks. In this article, we introduce WirelessAgent, a novel approach leveraging large language models (LLMs) to develop AI agents capable of managing complex tasks in wireless networks. It can effectively improve network performance through advanced reasoning, multimodal data processing, and autonomous decision making. Thereafter, we demonstrate the practical applicability and benefits of WirelessAgent for network slicing management. The experimental results show that WirelessAgent is capable of accurately understanding user intent, effectively allocating slice resources, and consistently maintaining optimal performance. }},
  url = {{ https://arxiv.org/abs/2409.07964v1 }}
}

@misc{Objection Overruled! Lay People can Distinguish Large Language Models_  from Lawyers, but still Favour Advice from an LLM_2024,
  title = {{ Objection Overruled! Lay People can Distinguish Large Language Models
  from Lawyers, but still Favour Advice from an LLM }},
  author = {{ Eike Schneiders, Tina Seabrooke, Joshua Krook, Richard Hyde, Natalie Leesakul, Jeremie Clos, Joel Fischer }},
  year = {{ 2024 }},
  eprint = {{ 2409.07871v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Large Language Models (LLMs) are seemingly infiltrating every domain, and the legal context is no exception. In this paper, we present the results of three experiments (total N=288) that investigated lay people's willingness to act upon, and their ability to discriminate between, LLM- and lawyer-generated legal advice. In Experiment 1, participants judged their willingness to act on legal advice when the source of the advice was either known or unknown. When the advice source was unknown, participants indicated that they were significantly more willing to act on the LLM-generated advice. This result was replicated in Experiment 2. Intriguingly, despite participants indicating higher willingness to act on LLM-generated advice in Experiments 1 and 2, participants discriminated between the LLM- and lawyer-generated texts significantly above chance-level in Experiment 3. Lastly, we discuss potential explanations and risks of our findings, limitations and future work, and the importance of language complexity and real-world comparability. }},
  url = {{ https://arxiv.org/abs/2409.07871v1 }}
}

@misc{Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs__  A Case Study in WeChat_2024,
  title = {{ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs:
  A Case Study in WeChat }},
  author = {{ Sidong Feng, Haochuan Lu, Jianqin Jiang, Ting Xiong, Likun Huang, Yinglin Liang, Xiaoqin Li, Yuetang Deng, Aldeida Aleti }},
  year = {{ 2024 }},
  eprint = {{ 2409.07829v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ UI automation tests play a crucial role in ensuring the quality of mobile applications. Despite the growing popularity of machine learning techniques to generate these tests, they still face several challenges, such as the mismatch of UI elements. The recent advances in Large Language Models (LLMs) have addressed these issues by leveraging their semantic understanding capabilities. However, a significant gap remains in applying these models to industrial-level app testing, particularly in terms of cost optimization and knowledge limitation. To address this, we introduce CAT to create cost-effective UI automation tests for industry apps by combining machine learning and LLMs with best practices. Given the task description, CAT employs Retrieval Augmented Generation (RAG) to source examples of industrial app usage as the few-shot learning context, assisting LLMs in generating the specific sequence of actions. CAT then employs machine learning techniques, with LLMs serving as a complementary optimizer, to map the target element on the UI screen. Our evaluations on the WeChat testing dataset demonstrate the CAT's performance and cost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming the state-of-the-art. We have also integrated our approach into the real-world WeChat testing platform, demonstrating its usefulness in detecting 141 bugs and enhancing the developers' testing process. }},
  url = {{ https://arxiv.org/abs/2409.07829v1 }}
}

@misc{Full-text Error Correction for Chinese Speech Recognition with Large_  Language Model_2024,
  title = {{ Full-text Error Correction for Chinese Speech Recognition with Large
  Language Model }},
  author = {{ Zhiyuan Tang, Dong Wang, Shen Huang, Shidong Shang }},
  year = {{ 2024 }},
  eprint = {{ 2409.07790v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Large Language Models (LLMs) have demonstrated substantial potential for error correction in Automatic Speech Recognition (ASR). However, most research focuses on utterances from short-duration speech recordings, which are the predominant form of speech data for supervised ASR training. This paper investigates the effectiveness of LLMs for error correction in full-text generated by ASR systems from longer speech recordings, such as transcripts from podcasts, news broadcasts, and meetings. First, we develop a Chinese dataset for full-text error correction, named ChFT, utilizing a pipeline that involves text-to-speech synthesis, ASR, and error-correction pair extractor. This dataset enables us to correct errors across contexts, including both full-text and segment, and to address a broader range of error types, such as punctuation restoration and inverse text normalization, thus making the correction process comprehensive. Second, we fine-tune a pre-trained LLM on the constructed dataset using a diverse set of prompts and target formats, and evaluate its performance on full-text error correction. Specifically, we design prompts based on full-text and segment, considering various output formats, such as directly corrected text and JSON-based error-correction pairs. Through various test settings, including homogeneous, up-to-date, and hard test sets, we find that the fine-tuned LLMs perform well in the full-text setting with different prompts, each presenting its own strengths and weaknesses. This establishes a promising baseline for further research. The dataset is available on the website. }},
  url = {{ https://arxiv.org/abs/2409.07790v1 }}
}

@misc{Alignment with Preference Optimization Is All You Need for LLM Safety_2024,
  title = {{ Alignment with Preference Optimization Is All You Need for LLM Safety }},
  author = {{ Reda Alami, Ali Khalifa Almansoori, Ahmed Alzubaidi, Mohamed El Amine Seddik, Mugariya Farooq, Hakim Hacid }},
  year = {{ 2024 }},
  eprint = {{ 2409.07772v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ We demonstrate that preference optimization methods can effectively enhance LLM safety. Applying various alignment techniques to the Falcon 11B model using safety datasets, we achieve a significant boost in global safety score (from $57.64\%$ to $99.90\%$) as measured by LlamaGuard 3 8B, competing with state-of-the-art models. On toxicity benchmarks, average scores in adversarial settings dropped from over $0.6$ to less than $0.07$. However, this safety improvement comes at the cost of reduced general capabilities, particularly in math, suggesting a trade-off. We identify noise contrastive alignment (Safe-NCA) as an optimal method for balancing safety and performance. Our study ultimately shows that alignment techniques can be sufficient for building safe and robust models. }},
  url = {{ https://arxiv.org/abs/2409.07772v1 }}
}

@misc{Ruri_ Japanese General Text Embeddings_2024,
  title = {{ Ruri: Japanese General Text Embeddings }},
  author = {{ Hayato Tsukagoshi, Ryohei Sasano }},
  year = {{ 2024 }},
  eprint = {{ 2409.07737v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ We report the development of Ruri, a series of Japanese general text embedding models. While the development of general-purpose text embedding models in English and multilingual contexts has been active in recent years, model development in Japanese remains insufficient. The primary reasons for this are the lack of datasets and the absence of necessary expertise. In this report, we provide a detailed account of the development process of Ruri. Specifically, we discuss the training of embedding models using synthesized datasets generated by LLMs, the construction of the reranker for dataset filtering and knowledge distillation, and the performance evaluation of the resulting general-purpose text embedding models. }},
  url = {{ https://arxiv.org/abs/2409.07737v1 }}
}

@misc{Large Language Models are Pattern Matchers_ Editing Semi-Structured and_  Structured Documents with ChatGPT_2024,
  title = {{ Large Language Models are Pattern Matchers: Editing Semi-Structured and
  Structured Documents with ChatGPT }},
  author = {{ Irene Weber }},
  year = {{ 2024 }},
  eprint = {{ 2409.07732v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Large Language Models (LLMs) offer numerous applications, the full extent of which is not yet understood. This paper investigates if LLMs can be applied for editing structured and semi-structured documents with minimal effort. Using a qualitative research approach, we conduct two case studies with ChatGPT and thoroughly analyze the results. Our experiments indicate that LLMs can effectively edit structured and semi-structured documents when provided with basic, straightforward prompts. ChatGPT demonstrates a strong ability to recognize and process the structure of annotated documents. This suggests that explicitly structuring tasks and data in prompts might enhance an LLM's ability to understand and solve tasks. Furthermore, the experiments also reveal impressive pattern matching skills in ChatGPT. This observation deserves further investigation, as it may contribute to understanding the processes leading to hallucinations in LLMs. }},
  url = {{ https://arxiv.org/abs/2409.07732v1 }}
}

@misc{DSBench_ How Far Are Data Science Agents to Becoming Data Science_  Experts_2024,
  title = {{ DSBench: How Far Are Data Science Agents to Becoming Data Science
  Experts? }},
  author = {{ Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu }},
  year = {{ 2024 }},
  eprint = {{ 2409.07703v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ cs.CL }},
  abstract = {{ Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents. }},
  url = {{ https://arxiv.org/abs/2409.07703v1 }}
}