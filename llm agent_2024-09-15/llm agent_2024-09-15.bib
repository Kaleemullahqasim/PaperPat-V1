@misc{Windows Agent Arena_ Evaluating Multi-Modal OS Agents at Scale_2024,
  title = {{ Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale }},
  author = {{ Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Justin Wagle, Kazuhito Koishida, Arthur Bucker, Lawrence Jang, Zack Hui }},
  year = {{ 2024 }},
  eprint = {{ 2409.08264v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ Large language models (LLMs) show remarkable potential to act as computer
agents, enhancing human productivity and software accessibility in multi-modal
tasks that require planning and reasoning. However, measuring agent performance
in realistic environments remains a challenge since: (i) most benchmarks are
limited to specific modalities or domains (e.g. text-only, web navigation, Q&A,
coding) and (ii) full benchmark evaluations are slow (on order of magnitude of
days) given the multi-step sequential nature of tasks. To address these
challenges, we introduce the Windows Agent Arena: a reproducible, general
environment focusing exclusively on the Windows operating system (OS) where
agents can operate freely within a real Windows OS and use the same wide range
of applications, tools, and web browsers available to human users when solving
tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse
Windows tasks across representative domains that require agent abilities in
planning, screen understanding, and tool usage. Our benchmark is scalable and
can be seamlessly parallelized in Azure for a full benchmark evaluation in as
little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we
also introduce a new multi-modal agent, Navi. Our agent achieves a success rate
of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted
human. Navi also demonstrates strong performance on another popular web-based
benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis
of Navi's performance, and provide insights into the opportunities for future
research in agent development and data generation using Windows Agent Arena.
  Webpage: https://microsoft.github.io/WindowsAgentArena
  Code: https://github.com/microsoft/WindowsAgentArena }},
  url = {{ https://arxiv.org/abs/2409.08264v1 }}
}

@misc{OmniQuery_ Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering_2024,
  title = {{ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering }},
  author = {{ Jiahao Nick Li, Zhuohao Jerry Zhang, Jiaju Ma }},
  year = {{ 2024 }},
  eprint = {{ 2409.08250v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ People often capture memories through photos, screenshots, and videos. While
existing AI-based tools enable querying this data using natural language, they
mostly only support retrieving individual pieces of information like certain
objects in photos and struggle with answering more complex queries that involve
interpreting interconnected memories like event sequences. We conducted a
one-month diary study to collect realistic user queries and generated a
taxonomy of necessary contextual information for integrating with captured
memories. We then introduce OmniQuery, a novel system that is able to answer
complex personal memory-related questions that require extracting and inferring
contextual information. OmniQuery augments single captured memories through
integrating scattered contextual information from multiple interconnected
memories, retrieves relevant memories, and uses a large language model (LLM) to
comprehensive answers. In human evaluations, we show the effectiveness of
OmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG
system, winning or tying in 74.5% of the time. }},
  url = {{ https://arxiv.org/abs/2409.08250v1 }}
}

@misc{Source2Synth_ Synthetic Data Generation and Curation Grounded in Real Data Sources_2024,
  title = {{ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources }},
  author = {{ Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli }},
  year = {{ 2024 }},
  eprint = {{ 2409.08239v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ Large Language Models still struggle in challenging scenarios that leverage
structured data, complex reasoning, or tool usage. In this paper, we propose
Source2Synth: a new method that can be used for teaching LLMs new skills
without relying on costly human annotations. Source2Synth takes as input a
custom data source and produces synthetic data points with intermediate
reasoning steps grounded in real-world sources. Source2Synth improves the
dataset quality by discarding low-quality generations based on their
answerability. We demonstrate the generality of this approach by applying it to
two challenging domains: we test reasoning abilities in multi-hop question
answering (MHQA), and tool usage in tabular question answering (TQA). Our
method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on
HotPotQA compared to the fine-tuned baselines. }},
  url = {{ https://arxiv.org/abs/2409.08239v1 }}
}

@misc{How can the tragedy of the commons be prevented__ Introducing Linear Quadratic Mixed Mean Field Games_2024,
  title = {{ How can the tragedy of the commons be prevented?: Introducing Linear Quadratic Mixed Mean Field Games }},
  author = {{ Gokce Dayanikli, Mathieu Lauriere }},
  year = {{ 2024 }},
  eprint = {{ 2409.08235v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ In a regular mean field game (MFG), the agents are assumed to be
insignificant, they do not realize their effect on the population level and
this may result in a phenomenon coined as the Tragedy of the Commons by the
economists. However, in real life this phenomenon is often avoided thanks to
the underlying altruistic behavior of (all or some of the) agents. Motivated by
this observation, we introduce and analyze two different mean field models to
include altruism in the decision making of agents. In the first model, mixed
individual MFGs, there are infinitely many agents who are partially altruistic
(i.e., they behave partially cooperatively) and partially non-cooperative. In
the second model, mixed population MFGs, one part of the population behaves
cooperatively and the remaining agents behave non-cooperatively. Both models
are introduced in a general linear quadratic framework for which we
characterize the equilibrium via forward backward stochastic differential
equations. Furthermore, we give explicit solutions in terms of ordinary
differential equations, and prove the existence and uniqueness results. }},
  url = {{ https://arxiv.org/abs/2409.08235v1 }}
}

@misc{LLM Honeypot_ Leveraging Large Language Models as Advanced Interactive Honeypot Systems_2024,
  title = {{ LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems }},
  author = {{ Hakan T. Otal, M. Abdullah Canbaz }},
  year = {{ 2024 }},
  eprint = {{ 2409.08234v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ The rapid evolution of cyber threats necessitates innovative solutions for
detecting and analyzing malicious activity. Honeypots, which are decoy systems
designed to lure and interact with attackers, have emerged as a critical
component in cybersecurity. In this paper, we present a novel approach to
creating realistic and interactive honeypot systems using Large Language Models
(LLMs). By fine-tuning a pre-trained open-source language model on a diverse
dataset of attacker-generated commands and responses, we developed a honeypot
capable of sophisticated engagement with attackers. Our methodology involved
several key steps: data collection and processing, prompt engineering, model
selection, and supervised fine-tuning to optimize the model's performance.
Evaluation through similarity metrics and live deployment demonstrated that our
approach effectively generates accurate and informative responses. The results
highlight the potential of LLMs to revolutionize honeypot technology, providing
cybersecurity professionals with a powerful tool to detect and analyze
malicious activity, thereby enhancing overall security infrastructure. }},
  url = {{ https://arxiv.org/abs/2409.08234v1 }}
}

@misc{Fine-tuning Large Language Models for Entity Matching_2024,
  title = {{ Fine-tuning Large Language Models for Entity Matching }},
  author = {{ Aaron Steiner, Ralph Peeters, Christian Bizer }},
  year = {{ 2024 }},
  eprint = {{ 2409.08185v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ Generative large language models (LLMs) are a promising alternative to
pre-trained language models for entity matching due to their high zero-shot
performance and their ability to generalize to unseen entities. Existing
research on using LLMs for entity matching has focused on prompt engineering
and in-context learning. This paper explores the potential of fine-tuning LLMs
for entity matching. We analyze fine-tuning along two dimensions: 1) The
representation of training examples, where we experiment with adding different
types of LLM-generated explanations to the training set, and 2) the selection
and generation of training examples using LLMs. In addition to the matching
performance on the source dataset, we investigate how fine-tuning affects the
model's ability to generalize to other in-domain datasets as well as across
topical domains. Our experiments show that fine-tuning significantly improves
the performance of the smaller models while the results for the larger models
are mixed. Fine-tuning also improves the generalization to in-domain datasets
while hurting cross-domain transfer. We show that adding structured
explanations to the training set has a positive impact on the performance of
three out of four LLMs, while the proposed example selection and generation
methods only improve the performance of Llama 3.1 8B while decreasing the
performance of GPT-4o Mini. }},
  url = {{ https://arxiv.org/abs/2409.08185v1 }}
}

@misc{Faster Speech-LLaMA Inference with Multi-token Prediction_2024,
  title = {{ Faster Speech-LLaMA Inference with Multi-token Prediction }},
  author = {{ Desh Raj, Gil Keren, Junteng Jia, Jay Mahadeokar, Ozlem Kalinli }},
  year = {{ 2024 }},
  eprint = {{ 2409.08148v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ Large language models (LLMs) have become proficient at solving a wide variety
of tasks, including those involving multi-modal inputs. In particular,
instantiating an LLM (such as LLaMA) with a speech encoder and training it on
paired data imparts speech recognition (ASR) abilities to the decoder-only
model, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of
auto-regressive inference and the relatively large decoder, Speech-LLaMA models
require relatively high inference time. In this work, we propose to speed up
Speech-LLaMA inference by predicting multiple tokens in the same decoding step.
We explore several model architectures that enable this, and investigate their
performance using threshold-based and verification-based inference strategies.
We also propose a prefix-based beam search decoding method that allows
efficient minimum word error rate (MWER) training for such models. We evaluate
our models on a variety of public benchmarks, where they reduce the number of
decoder calls by ~3.2x while maintaining or improving WER performance. }},
  url = {{ https://arxiv.org/abs/2409.08148v1 }}
}

@misc{LLM-POTUS Score_ A Framework of Analyzing Presidential Debates with Large Language Models_2024,
  title = {{ LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models }},
  author = {{ Zhengliang Liu, Yiwei Li, Oleksandra Zolotarevych, Rongwei Yang, Tianming Liu }},
  year = {{ 2024 }},
  eprint = {{ 2409.08147v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ Large language models have demonstrated remarkable capabilities in natural
language processing, yet their application to political discourse analysis
remains underexplored. This paper introduces a novel approach to evaluating
presidential debate performances using LLMs, addressing the longstanding
challenge of objectively assessing debate outcomes. We propose a framework that
analyzes candidates' "Policies, Persona, and Perspective" (3P) and how they
resonate with the "Interests, Ideologies, and Identity" (3I) of four key
audience groups: voters, businesses, donors, and politicians. Our method
employs large language models to generate the LLM-POTUS Score, a quantitative
measure of debate performance based on the alignment between 3P and 3I. We
apply this framework to analyze transcripts from recent U.S. presidential
debates, demonstrating its ability to provide nuanced, multi-dimensional
assessments of candidate performances. Our results reveal insights into the
effectiveness of different debating strategies and their impact on various
audience segments. This study not only offers a new tool for political analysis
but also explores the potential and limitations of using LLMs as impartial
judges in complex social contexts. In addition, this framework provides
individual citizens with an independent tool to evaluate presidential debate
performances, which enhances democratic engagement and reduces reliance on
potentially biased media interpretations and institutional influence, thereby
strengthening the foundation of informed civic participation. }},
  url = {{ https://arxiv.org/abs/2409.08147v1 }}
}

@misc{Linear Complementary Dual Codes Constructed from Reinforcement Learning_2024,
  title = {{ Linear Complementary Dual Codes Constructed from Reinforcement Learning }},
  author = {{ Yansheng Wu, Jin Ma, Shandong Yang }},
  year = {{ 2024 }},
  eprint = {{ 2409.08114v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ Recently, Linear Complementary Dual (LCD) codes have garnered substantial
interest within coding theory research due to their diverse applications and
favorable attributes. This paper directs its attention to the construction of
binary and ternary LCD codes leveraging curiosity-driven reinforcement learning
(RL). By establishing reward and devising well-reasoned mappings from actions
to states, it aims to facilitate the successful synthesis of binary or ternary
LCD codes. Experimental results indicate that LCD codes constructed using RL
exhibit slightly superior error-correction performance compared to those
conventionally constructed LCD codes and those developed via standard RL
methodologies. The paper introduces novel binary and ternary LCD codes with
enhanced minimum distance bounds. Finally, it showcases how Random Network
Distillation aids agents in exploring beyond local optima, enhancing the
overall performance of the models without compromising convergence. }},
  url = {{ https://arxiv.org/abs/2409.08114v1 }}
}

@misc{The CLC-UKET Dataset_ Benchmarking Case Outcome Prediction for the UK Employment Tribunal_2024,
  title = {{ The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal }},
  author = {{ Huiyuan Xie, Felix Steffek, Joana Ribeiro de Faria, Christine Carter, Jonathan Rutherford }},
  year = {{ 2024 }},
  eprint = {{ 2409.08098v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ This paper explores the intersection of technological innovation and access
to justice by developing a benchmark for predicting case outcomes in the UK
Employment Tribunal (UKET). To address the challenge of extensive manual
annotation, the study employs a large language model (LLM) for automatic
annotation, resulting in the creation of the CLC-UKET dataset. The dataset
consists of approximately 19,000 UKET cases and their metadata. Comprehensive
legal annotations cover facts, claims, precedent references, statutory
references, case outcomes, reasons and jurisdiction codes. Facilitated by the
CLC-UKET data, we examine a multi-class case outcome prediction task in the
UKET. Human predictions are collected to establish a performance reference for
model comparison. Empirical results from baseline models indicate that
finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET
prediction task. The performance of zero-shot LLMs can be enhanced by
integrating task-related information into few-shot examples. We hope that the
CLC-UKET dataset, along with human annotations and empirical findings, can
serve as a valuable benchmark for employment-related dispute resolution. }},
  url = {{ https://arxiv.org/abs/2409.08098v1 }}
}

@misc{Securing Large Language Models_ Addressing Bias, Misinformation, and Prompt Attacks_2024,
  title = {{ Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks }},
  author = {{ Benji Peng, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Junyu Liu, Qian Niu }},
  year = {{ 2024 }},
  eprint = {{ 2409.08087v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ Large Language Models (LLMs) demonstrate impressive capabilities across
various fields, yet their increasing use raises critical security concerns.
This article reviews recent literature addressing key issues in LLM security,
with a focus on accuracy, bias, content detection, and vulnerability to
attacks. Issues related to inaccurate or misleading outputs from LLMs is
discussed, with emphasis on the implementation from fact-checking methodologies
to enhance response reliability. Inherent biases within LLMs are critically
examined through diverse evaluation techniques, including controlled input
studies and red teaming exercises. A comprehensive analysis of bias mitigation
strategies is presented, including approaches from pre-processing interventions
to in-training adjustments and post-processing refinements. The article also
probes the complexity of distinguishing LLM-generated content from
human-produced text, introducing detection mechanisms like DetectGPT and
watermarking techniques while noting the limitations of machine learning
enabled classifiers under intricate circumstances. Moreover, LLM
vulnerabilities, including jailbreak attacks and prompt injection exploits, are
analyzed by looking into different case studies and large-scale competitions
like HackAPrompt. This review is concluded by retrospecting defense mechanisms
to safeguard LLMs, accentuating the need for more extensive research into the
LLM security field. }},
  url = {{ https://arxiv.org/abs/2409.08087v1 }}
}

@misc{MosquitoMiner_ A Light Weight Rover for Detecting and Eliminating Mosquito Breeding Sites_2024,
  title = {{ MosquitoMiner: A Light Weight Rover for Detecting and Eliminating Mosquito Breeding Sites }},
  author = {{ Md. Adnanul Islam, Md. Faiyaz Abdullah Sayeedi, Jannatul Ferdous Deepti, Shahanur Rahman Bappy, Safrin Sanzida Islam, Fahim Hafiz }},
  year = {{ 2024 }},
  eprint = {{ 2409.08078v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ In this paper, we present a novel approach to the development and deployment
of an autonomous mosquito breeding place detector rover with the object and
obstacle detection capabilities to control mosquitoes. Mosquito-borne diseases
continue to pose significant health threats globally, with conventional control
methods proving slow and inefficient. Amidst rising concerns over the rapid
spread of these diseases, there is an urgent need for innovative and efficient
strategies to manage mosquito populations and prevent disease transmission. To
mitigate the limitations of manual labor and traditional methods, our rover
employs autonomous control strategies. Leveraging our own custom dataset, the
rover can autonomously navigate along a pre-defined path, identifying and
mitigating potential breeding grounds with precision. It then proceeds to
eliminate these breeding grounds by spraying a chemical agent, effectively
eradicating mosquito habitats. Our project demonstrates the effectiveness that
is absent in traditional ways of controlling and safeguarding public health.
The code for this project is available on GitHub at -
https://github.com/faiyazabdullah/MosquitoMiner }},
  url = {{ https://arxiv.org/abs/2409.08078v1 }}
}

@misc{TravelAgent_ An AI Assistant for Personalized Travel Planning_2024,
  title = {{ TravelAgent: An AI Assistant for Personalized Travel Planning }},
  author = {{ Aili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, Jiangjie Chen }},
  year = {{ 2024 }},
  eprint = {{ 2409.08069v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ As global tourism expands and artificial intelligence technology advances,
intelligent travel planning services have emerged as a significant research
focus. Within dynamic real-world travel scenarios with multi-dimensional
constraints, services that support users in automatically creating practical
and customized travel itineraries must address three key objectives:
Rationality, Comprehensiveness, and Personalization. However, existing systems
with rule-based combinations or LLM-based planning methods struggle to fully
satisfy these criteria. To overcome the challenges, we introduce TravelAgent, a
travel planning system powered by large language models (LLMs) designed to
provide reasonable, comprehensive, and personalized travel itineraries grounded
in dynamic scenarios. TravelAgent comprises four modules: Tool-usage,
Recommendation, Planning, and Memory Module. We evaluate TravelAgent's
performance with human and simulated users, demonstrating its overall
effectiveness in three criteria and confirming the accuracy of personalized
recommendations. }},
  url = {{ https://arxiv.org/abs/2409.08069v1 }}
}

@misc{From Explanations to Action_ A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback_2024,
  title = {{ From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback }},
  author = {{ Vinitra Swamy, Davide Romano, Bhargav Srinivasa Desikan, Oana-Maria Camburu, Tanja KÃ¤ser }},
  year = {{ 2024 }},
  eprint = {{ 2409.08027v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ Recent advances in eXplainable AI (XAI) for education have highlighted a
critical challenge: ensuring that explanations for state-of-the-art AI models
are understandable for non-technical users such as educators and students. In
response, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI
pipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is
designed to deliver theory-driven, actionable feedback to students in online
courses. iLLuMinaTE navigates three main stages - causal connection,
explanation selection, and explanation presentation - with variations drawing
from eight social science theories (e.g. Abnormal Conditions, Pearl's Model of
Explanation, Necessity and Robustness Selection, Contrastive Explanation). We
extensively evaluate 21,915 natural language explanations of iLLuMinaTE
extracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different
underlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from
three diverse online courses. Our evaluation involves analyses of explanation
alignment to the social science theory, understandability of the explanation,
and a real-world user preference study with 114 university students containing
a novel actionability simulation. We find that students prefer iLLuMinaTE
explanations over traditional explainers 89.52% of the time. Our work provides
a robust, ready-to-use framework for effectively communicating hybrid
XAI-driven insights in education, with significant generalization potential for
other human-centric fields. }},
  url = {{ https://arxiv.org/abs/2409.08027v1 }}
}

@misc{An Evaluation Framework for Attributed Information Retrieval using Large Language Models_2024,
  title = {{ An Evaluation Framework for Attributed Information Retrieval using Large Language Models }},
  author = {{ Hanane Djeddal, Pierre Erbacher, Raouf Toukal, Laure Soulier, Karen Pinel-Sauvagnat, Sophia Katrenko, Lynda Tamine }},
  year = {{ 2024 }},
  eprint = {{ 2409.08014v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ With the growing success of Large Language models (LLMs) in
information-seeking scenarios, search engines are now adopting generative
approaches to provide answers along with in-line citations as attribution.
While existing work focuses mainly on attributed question answering, in this
paper, we target information-seeking scenarios which are often more challenging
due to the open-ended nature of the queries and the size of the label space in
terms of the diversity of candidate-attributed answers per query. We propose a
reproducible framework to evaluate and benchmark attributed information
seeking, using any backbone LLM, and different architectural designs: (1)
Generate (2) Retrieve then Generate, and (3) Generate then Retrieve.
Experiments using HAGRID, an attributed information-seeking dataset, show the
impact of different scenarios on both the correctness and attributability of
answers. }},
  url = {{ https://arxiv.org/abs/2409.08014v1 }}
}

@misc{Learning Communities from Equilibria of Nonlinear Opinion Dynamics_2024,
  title = {{ Learning Communities from Equilibria of Nonlinear Opinion Dynamics }},
  author = {{ Yu Xing, Anastasia Bizyaeva, Karl H. Johansson }},
  year = {{ 2024 }},
  eprint = {{ 2409.08004v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ This paper studies community detection for a nonlinear opinion dynamics model
from its equilibria. It is assumed that the underlying network is generated
from a stochastic block model with two communities, where agents are assigned
with community labels and edges are added independently based on these labels.
Agents update their opinions following a nonlinear rule that incorporates
saturation effects on interactions. It is shown that clustering based on a
single equilibrium can detect most community labels (i.e., achieving almost
exact recovery), if the two communities differ in size and link probabilities.
When the two communities are identical in size and link probabilities, and the
inter-community connections are denser than intra-community ones, the algorithm
can achieve almost exact recovery under negative influence weights but fails
under positive influence weights. Utilizing the fixed point equation and
spectral methods, we also propose a detection algorithm based on multiple
equilibria, which can detect communities with positive influence weights.
Numerical experiments demonstrate the performance of the proposed algorithms. }},
  url = {{ https://arxiv.org/abs/2409.08004v1 }}
}

@misc{Autonomous Vehicle Controllers From End-to-End Differentiable Simulation_2024,
  title = {{ Autonomous Vehicle Controllers From End-to-End Differentiable Simulation }},
  author = {{ Asen Nachkov, Danda Pani Paudel, Luc Van Gool }},
  year = {{ 2024 }},
  eprint = {{ 2409.07965v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ Current methods to learn controllers for autonomous vehicles (AVs) focus on
behavioural cloning. Being trained only on exact historic data, the resulting
agents often generalize poorly to novel scenarios. Simulators provide the
opportunity to go beyond offline datasets, but they are still treated as
complicated black boxes, only used to update the global simulation state. As a
result, these RL algorithms are slow, sample-inefficient, and prior-agnostic.
In this work, we leverage a differentiable simulator and design an analytic
policy gradients (APG) approach to training AV controllers on the large-scale
Waymo Open Motion Dataset. Our proposed framework brings the differentiable
simulator into an end-to-end training loop, where gradients of the environment
dynamics serve as a useful prior to help the agent learn a more grounded
policy. We combine this setup with a recurrent architecture that can
efficiently propagate temporal information across long simulated trajectories.
This APG method allows us to learn robust, accurate, and fast policies, while
only requiring widely-available expert trajectories, instead of scarce expert
actions. We compare to behavioural cloning and find significant improvements in
performance and robustness to noise in the dynamics, as well as overall more
intuitive human-like handling. }},
  url = {{ https://arxiv.org/abs/2409.07965v1 }}
}

@misc{WirelessAgent_ Large Language Model Agents for Intelligent Wireless Networks_2024,
  title = {{ WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks }},
  author = {{ Jingwen Tong, Jiawei Shao, Qiong Wu, Wei Guo, Zijian Li, Zehong Lin, Jun Zhang }},
  year = {{ 2024 }},
  eprint = {{ 2409.07964v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ Wireless networks are increasingly facing challenges due to their expanding
scale and complexity. These challenges underscore the need for advanced
AI-driven strategies, particularly in the upcoming 6G networks. In this
article, we introduce WirelessAgent, a novel approach leveraging large language
models (LLMs) to develop AI agents capable of managing complex tasks in
wireless networks. It can effectively improve network performance through
advanced reasoning, multimodal data processing, and autonomous decision making.
Thereafter, we demonstrate the practical applicability and benefits of
WirelessAgent for network slicing management. The experimental results show
that WirelessAgent is capable of accurately understanding user intent,
effectively allocating slice resources, and consistently maintaining optimal
performance. }},
  url = {{ https://arxiv.org/abs/2409.07964v1 }}
}

@misc{Covariance Intersection-based Invariant Kalman Filtering(DInCIKF) for Distributed Pose Estimation_2024,
  title = {{ Covariance Intersection-based Invariant Kalman Filtering(DInCIKF) for Distributed Pose Estimation }},
  author = {{ Haoying Li, Xinghan Li, Shuaiting Huang, Chao yang, Junfeng Wu }},
  year = {{ 2024 }},
  eprint = {{ 2409.07933v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ This paper presents a novel approach to distributed pose estimation in the
multi-agent system based on an invariant Kalman filter with covariance
intersection. Our method models uncertainties using Lie algebra and applies
object-level observations within Lie groups, which have practical application
value. We integrate covariance intersection to handle estimates that are
correlated and use the invariant Kalman filter for merging independent data
sources. This strategy allows us to effectively tackle the complex correlations
of cooperative localization among agents, ensuring our estimates are neither
too conservative nor overly confident. Additionally, we examine the consistency
and stability of our algorithm, providing evidence of its reliability and
effectiveness in managing multi-agent systems. }},
  url = {{ https://arxiv.org/abs/2409.07933v1 }}
}

@misc{Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies_2024,
  title = {{ Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies }},
  author = {{ Alexei Pisacane, Victor-Alexandru Darvariu, Mirco Musolesi }},
  year = {{ 2024 }},
  eprint = {{ 2409.07932v1 }},
  archivePrefix = {arXiv},
  primaryClass = {{ N/A }},
  abstract = {{ Graph path search is a classic computer science problem that has been
recently approached with Reinforcement Learning (RL) due to its potential to
outperform prior methods. Existing RL techniques typically assume a global view
of the network, which is not suitable for large-scale, dynamic, and
privacy-sensitive settings. An area of particular interest is search in social
networks due to its numerous applications. Inspired by seminal work in
experimental sociology, which showed that decentralized yet efficient search is
possible in social networks, we frame the problem as a collaborative task
between multiple agents equipped with a limited local view of the network. We
propose a multi-agent approach for graph path search that successfully
leverages both homophily and structural heterogeneity. Our experiments, carried
out over synthetic and real-world social networks, demonstrate that our model
significantly outperforms learned and heuristic baselines. Furthermore, our
results show that meaningful embeddings for graph navigation can be constructed
using reward-driven learning. }},
  url = {{ https://arxiv.org/abs/2409.07932v1 }}
}